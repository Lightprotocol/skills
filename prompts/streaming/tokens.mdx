<Prompt description="Stream light-token accounts via Laserstream gRPC" actions={["copy", "cursor"]}>
{`---
description: Stream light-token accounts via Laserstream gRPC
allowed-tools: Bash, Read, Write, Edit, Glob, Grep, WebFetch, AskUserQuestion, Task, TaskCreate, TaskGet, TaskList, TaskUpdate, TaskOutput, mcp__deepwiki, mcp__zkcompression
---

## Stream light-token accounts via Laserstream gRPC

Context:
- Guide: https://zkcompression.com/light-token/toolkits/for-streaming-tokens
- Skills and resources index: https://zkcompression.com/skill.md
- Dedicated skill: https://github.com/Lightprotocol/skills/tree/main/skills/data-streaming
- Crates: helius-laserstream, light-token-interface, spl-pod, spl-token-2022-interface, borsh, futures
- Mint accounts streaming: https://zkcompression.com/light-token/toolkits/for-streaming-mints
- Point queries: light-client (LightClient, get_account_interface)

Key APIs: LaserstreamConfig, subscribe(), PodAccount (pod_from_bytes), LightClient::get_account_interface()

### 1. Index project
- Grep \`helius_laserstream|laserstream|subscribe|PodAccount|pod_from_bytes|spl_token_2022_interface|cTokenmWW8bLPjZEBAUgYy3zKxQZW6VKi7bqNFEVv3m\` across src/
- Glob \`**/*.rs\` and \`**/Cargo.toml\` for project structure
- Identify: existing gRPC streaming setup, token account caching, SPL parser usage
- Read Cargo.toml — note existing dependencies
- Task subagent (Grep/Read/WebFetch) if project has multiple crates to scan in parallel

### 2. Read references
- WebFetch the guide above — follow the Steps (Connect, Subscribe) and the transition detection sections
- WebFetch skill.md — check for a dedicated skill and resources matching this task
- TaskCreate one todo per phase below to track progress

### 3. Clarify intention
- AskUserQuestion: what is the goal? (new streaming pipeline for token accounts, add to existing pipeline, integrate cold/hot detection for routing)
- AskUserQuestion: mainnet or devnet?
- AskUserQuestion: do you need point queries (get_account_interface) in addition to streaming?
- Summarize findings and wait for user confirmation before implementing

### 4. Create plan
- Based on steps 1–3, draft an implementation plan
- Follow the guide's structure: Connect → Subscribe (account + transaction subs) → Detect Transitions (hot-to-cold, cold-to-hot) → Point Queries (optional)
- Token accounts use the same 165-byte SPL layout — existing SPL parsers work directly
- If anything is unclear or ambiguous, loop back to step 3 (AskUserQuestion)
- Present the plan to the user for approval before proceeding

### 5. Implement
- Add deps if missing: Bash \`cargo add helius-laserstream@0.1 light-token-interface@0.3 spl-pod spl-token-2022-interface borsh@0.10 futures@0.3 bs58@0.5 tokio --features full\`
- For point queries, also: Bash \`cargo add light-client@0.19 --features v2\`
- Follow the guide and the approved plan
- Write/Edit to create or modify files
- TaskUpdate to mark each step done

### 6. Verify
- Bash \`cargo check\`
- Bash \`cargo test\` if tests exist
- TaskUpdate to mark complete

### Tools
- mcp__zkcompression__SearchLightProtocol("<query>") for API details
- mcp__deepwiki__ask_question("Lightprotocol/light-protocol", "<q>") for architecture
- Task subagent with Grep/Read/WebFetch for parallel lookups
- TaskList to check remaining work`}
</Prompt>

```text
---
description: Stream light-token accounts via Laserstream gRPC
allowed-tools: Bash, Read, Write, Edit, Glob, Grep, WebFetch, AskUserQuestion, Task, TaskCreate, TaskGet, TaskList, TaskUpdate, TaskOutput, mcp__deepwiki, mcp__zkcompression
---

## Stream light-token accounts via Laserstream gRPC

Context:
- Guide: https://zkcompression.com/light-token/toolkits/for-streaming-tokens
- Skills and resources index: https://zkcompression.com/skill.md
- Dedicated skill: https://github.com/Lightprotocol/skills/tree/main/skills/data-streaming
- Crates: helius-laserstream, light-token-interface, spl-pod, spl-token-2022-interface, borsh, futures
- Mint accounts streaming: https://zkcompression.com/light-token/toolkits/for-streaming-mints
- Point queries: light-client (LightClient, get_account_interface)

Key APIs: LaserstreamConfig, subscribe(), PodAccount (pod_from_bytes), LightClient::get_account_interface()

### 1. Index project
- Grep `helius_laserstream|laserstream|subscribe|PodAccount|pod_from_bytes|spl_token_2022_interface|cTokenmWW8bLPjZEBAUgYy3zKxQZW6VKi7bqNFEVv3m` across src/
- Glob `**/*.rs` and `**/Cargo.toml` for project structure
- Identify: existing gRPC streaming setup, token account caching, SPL parser usage
- Read Cargo.toml — note existing dependencies
- Task subagent (Grep/Read/WebFetch) if project has multiple crates to scan in parallel

### 2. Read references
- WebFetch the guide above — follow the Steps (Connect, Subscribe) and the transition detection sections
- WebFetch skill.md — check for a dedicated skill and resources matching this task
- TaskCreate one todo per phase below to track progress

### 3. Clarify intention
- AskUserQuestion: what is the goal? (new streaming pipeline for token accounts, add to existing pipeline, integrate cold/hot detection for routing)
- AskUserQuestion: mainnet or devnet?
- AskUserQuestion: do you need point queries (get_account_interface) in addition to streaming?
- Summarize findings and wait for user confirmation before implementing

### 4. Create plan
- Based on steps 1–3, draft an implementation plan
- Follow the guide's structure: Connect → Subscribe (account + transaction subs) → Detect Transitions (hot-to-cold, cold-to-hot) → Point Queries (optional)
- Token accounts use the same 165-byte SPL layout — existing SPL parsers work directly
- If anything is unclear or ambiguous, loop back to step 3 (AskUserQuestion)
- Present the plan to the user for approval before proceeding

### 5. Implement
- Add deps if missing: Bash `cargo add helius-laserstream@0.1 light-token-interface@0.3 spl-pod spl-token-2022-interface borsh@0.10 futures@0.3 bs58@0.5 tokio --features full`
- For point queries, also: Bash `cargo add light-client@0.19 --features v2`
- Follow the guide and the approved plan
- Write/Edit to create or modify files
- TaskUpdate to mark each step done

### 6. Verify
- Bash `cargo check`
- Bash `cargo test` if tests exist
- TaskUpdate to mark complete

### Tools
- mcp__zkcompression__SearchLightProtocol("<query>") for API details
- mcp__deepwiki__ask_question("Lightprotocol/light-protocol", "<q>") for architecture
- Task subagent with Grep/Read/WebFetch for parallel lookups
- TaskList to check remaining work
```
